{
  "hash": "8fa6c33f03c61fda7a6a8f12c8a4d33d",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Stochastic log volatility\n\n## AIRG documentation and reported results\n\n### Documents\n\n**This is the key document:**\n\n“Recommended Approach for Setting Regulatory Risk-Based Capital Requirements for Variable Annuities and Similar Products Presented by the American Academy of Actuaries’ Life Capital Adequacy Subcommittee to the National Association of Insurance Commissioners’ Capital Adequacy Task Force.” American Academy of Actuaries, June 2005. <https://www.actuary.org/sites/default/files/pdf/life/c3_june05.pdf>.\n\n![](images/clipboard-2233928360.png)\n\nAlso useful:\n\n“C3 Phase II Risk-Based Capital for Variable Annuities: Pre-Packaged Scenarios:  Presented by the American Academy of Actuaries’ Life Capital Adequacy Subcommittees’ C-3 Phase 2 Work Group to the National Association of Insurance Commissioners’ Capital Adequacy Task Force.” American Academy of Actuaries, March 2005. <https://www.actuary.org/sites/default/files/pdf/life/c3supp_march05.pdf>.\n\n### AIRG parameters\n\n![](images/clipboard-1062302077.png)\n\n![](images/clipboard-239653750.png)\n\n### AIRG SLV equations\n\n![](images/clipboard-325100188.png)\n\n## Setup\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"report\", \"_common.R\"))\nsource(here::here(\"report\", \"libraries_ts.R\"))\n\n\nlibrary(quantmod)\nlibrary(stats4)\nlibrary(stochvolTMB)\nlibrary(stochvol)\nlibrary(rstan)\nlibrary(V8)\n\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE) # avoids recompiling\n```\n:::\n\n\n\n\n## Get data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dec 1955-Dec 2003 monthly was period used in AIRG for sp500\n\n# Download adjusted close prices for the S&P 500 index\n# monthly data do not go back far enough so get daily and convert\n\n# re-download if needed\nspdaily <- getSymbols(\"^GSPC\", auto.assign = FALSE, from = \"1950-01-01\")\nsaveRDS(spdaily, here::here(\"data\", \"sp500_index_rawdaily.rds\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# get previously downloaded and saved data\n\nspdaily <- readRDS(here::here(\"data\", \"sp500_index_rawdaily.rds\"))\n\nspmonthly <- spdaily |> \n  to.monthly(OHLC=FALSE) |> \n  fortify.zoo() |> # to dataframe with Index from rowname\n  select(date=Index, sp500=GSPC.Adjusted) |> \n  # we can safely assume data are sorted by date\n  mutate(lr=log(sp500 / lag(sp500))) # note that we lose lr for the first observation\n\n# skim(spmonthly)\n```\n:::\n\n\n\n\n## Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://github.com/m-clark/models-by-example/blob/main/bayesian-stochastic-volatility.Rmd\n# https://discourse.mc-stan.org/t/adapt-delta/7947/2\n# https://groups.google.com/g/stan-users/c/zNttu0wgavg?pli=1\n# https://mc-stan.org/rstanarm/reference/adapt_delta.html#references\n# https://mc-stan.org/docs/stan-users-guide/time-series.html#stochastic-volatility-models\n\n\n# get centered log returns - don't forget first obs in data is missing lr\nperiod <- c(\"1955-12-31\", \"2003-12-31\")\n# period <- c(\"1955-12-31\", \"2023-12-31\")\n# period <- c(\"1950-02-28\", \"2024-03-31\")\n\nlog_returns <- spmonthly |> \n  filter(date >= period[1],\n         date <= period[2]) |> \n  mutate(annlr=lr * 12, # annualized\n         mclr=scale(annlr, scale = FALSE) |> c()) |> # mean-centered log return\n  pull(mclr)\n\n\nstan_data <- list(\n  N_t = length(log_returns),\n  y = log_returns\n)\n\nsfile <- here::here(\"report\", \"slv_best.stan\") # best so far\n# sfile <- here::here(\"report\", \"slv1.stan\") # slow\n# sfile <- here::here(\"report\", \"slv2.stan\") # good but not always 67 secs\n\ncat(readLines(sfile), sep = \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n// https://github.com/m-clark/models-by-example/blob/main/bayesian-stochastic-volatility.Rmd\ndata {\n  int<lower = 0> N_t;                       // N time points (equally spaced)\n  vector[N_t] y;                            // mean corrected response at time t\n}\n\nparameters {\n  real mu;                                  // mean log volatility\n  real<lower = -1,upper = 1> phi;           // persistence of volatility\n  real<lower = 0> sigma;                    // white noise shock scale\n  vector[N_t] h_std;                        // standardized log volatility at time t\n}\n\ntransformed parameters{\n  vector[N_t] h;                            // log volatility at time t\n  \n  h    = h_std * sigma;\n  h[1] = h[1] / sqrt(1-phi * phi);\n  h = h + mu;\n  \n  for (t in 2:N_t)\n    h[t] = h[t] + phi * (h[t-1] - mu);\n}\n\nmodel {\n  //priors\n  phi   ~ uniform(-1, 1);\n  sigma ~ cauchy(0, 5);\n  mu    ~ cauchy(0, 10);\n  h_std ~ normal(0, 1);\n\n  //likelihood\n  y ~ normal(0, exp(h/2));\n}\n\ngenerated quantities{\n  vector[N_t] y_rep;\n\n  for (t in 1:N_t){\n    y_rep[t] = normal_rng(0, exp(h[t]/2));\n  }  \n}\n```\n\n\n:::\n\n```{.r .cell-code}\n# control <- list(adapt_delta=0.99, stepsize = 0.01, max_treedepth = 20)\n# control <- list(adapt_delta= 0.99, stepsize = 0.005, max_treedepth = 20)\n# control <- list(adapt_delta= 0.995, stepsize = 0.005, max_treedepth = 25)\n# control <- list(adapt_delta= 0.995, max_treedepth = 25)\ncontrol <- list(adapt_delta= 0.995)\n\nfit <- stan(\n  file = sfile,\n  data = stan_data,\n  control=control,\n  iter = 2000,\n  chains = 4\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrecompiling to avoid crashing R session\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\n  fit,\n  digits = 3,\n  par    = c('mu', 'phi', 'sigma'),\n  probs  = c(.025, .5, .975)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean    sd   2.5%    50%  97.5% n_eff  Rhat\nmu    -1.545   0.003 0.143 -1.824 -1.548 -1.260  2357 1.000\nphi    0.892   0.002 0.056  0.750  0.903  0.964   745 1.008\nsigma  0.282   0.002 0.068  0.171  0.276  0.435   882 1.005\n\nSamples were drawn using NUTS(diag_e) at Sat Apr 13 07:38:20 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n\n```{.r .cell-code}\n# compare, with stochvolTMB\n# svmod <- estimate_parameters(log_returns, model = \"gaussian\", silent = TRUE)\n# summary(svmod)\n```\n:::\n\n\n\n\n## AIRG reported results\n\n### AIRG sample (1955:12 - 2003:12)\n\n#### Annualized returns\n\n![](images/clipboard-1629488555.png)\n\n#### Monthly returns\n\n![](images/clipboard-3660888329.png)\n\n### AIRG + 20 years (1955:12 - 2023:12)\n\n![](images/clipboard-1127146836.png)\n\n### Full sample (1950:02 - 2024:03)\n\n![](images/clipboard-1292758530.png)\n\n## OLDER STUFF\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For execution on a local, multicore CPU with excess RAM we recommend calling\n# options(mc.cores = parallel::detectCores()).\n# To avoid recompilation of unchanged Stan programs, we recommend calling\n# rstan_options(auto_write = TRUE)\n# For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\n# change `threads_per_chain` option:\n# rstan_options(threads_per_chain = 1)\n# \n# Do not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n\n# prepare log returns in a list format that matches the data block in your Stan model\n\n\nlog_returns <- sp500b |> \n  filter(date >= \"1995-12-01\",\n         date <= \"2003-12-01\") |> \n  mutate(mclr=lr - mean(lr)) |> # mean center\n  pull(mclr)\n\ndata_list <- list(\n  T = length(log_returns),\n  y = log_returns\n)\n\nsfile <- here::here(\"report\", \"slv.stan\")\nsfile <- here::here(\"report\", \"slv2.stan\")\nsfile <- here::here(\"report\", \"slv3.stan\")\n\ncat(readLines(sfile), sep = \"\\n\")\n\n\n# Fit the model\nfit <- stan(\n  file = sfile,  # Your Stan model file\n  data = data_list,\n  iter = 2000,\n  chains = 4\n)\n\nfit\nstan_diag(fit)\nstan_trace(fit)\n\n\n# Warning messages:\n# 1: There were 2106 divergent transitions after warmup. See\n# https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n# to find out why this is a problem and how to eliminate them. \n# 2: There were 3 chains where the estimated Bayesian Fraction of Missing Information was low. See\n# https://mc-stan.org/misc/warnings.html#bfmi-low \n# 3: Examine the pairs() plot to diagnose sampling problems\n#  \n# 4: The largest R-hat is 2.06, indicating chains have not mixed.\n# Running the chains for more iterations may help. See\n# https://mc-stan.org/misc/warnings.html#r-hat \n# 5: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\n# Running the chains for more iterations may help. See\n# https://mc-stan.org/misc/warnings.html#bulk-ess \n# 6: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\n# Running the chains for more iterations may help. See\n# https://mc-stan.org/misc/warnings.html#tail-ess \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://github.com/m-clark/models-by-example/blob/main/bayesian-stochastic-volatility.Rmd\n# https://groups.google.com/g/stan-users/c/zNttu0wgavg?pli=1\n\nd <- read_csv(\n  'https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv',\n  col_names = 'inflation'\n)\n\ninflation <- pull(d, inflation)\nsummary(inflation)\n\ninflation_cen <- scale(inflation, scale = FALSE)\nstan_data = list(N_t = length(inflation_cen), y = c(inflation_cen))\n\nstan_data <- list(\n  N_t = length(log_returns),\n  y = log_returns\n)\n\nsfile <- here::here(\"report\", \"infl1.stan\")\nsfile <- here::here(\"report\", \"infl2.stan\")\n\n# fit = sampling(\n#   file = sfile,\n#   data  = stan_data,\n#   cores = 4,\n#   thin  = 4\n# )\n\na <- proc.time()\nfit <- stan(\n  file = sfile,  # Your Stan model file\n  data = stan_data,\n  control=list(adapt_delta=0.99, stepsize = 0.01, max_treedepth =20),\n  iter = 2000,\n  chains = 4\n)\nb <- proc.time()\nb - a\n\n\nprint(\n  fit,\n  digits = 3,\n  par    = c('mu', 'phi', 'sigma'),\n  probs  = c(.025, .5, .975)\n)\n\n# stan documentation\n# example(stan_model, package = \"rstan\", run.dontrun = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(spy)\nplot(spy$date, spy$log_return, type = \"l\", xlab = \"\", ylab = \"\", main = \"Log-returns of S&P500\")\nplot(spy$date, spy$price, type = \"l\", xlab = \"\", ylab = \"\", main = \"Price of S&P500\")\n\n\ngaussian = estimate_parameters(spy$log_return, model = \"gaussian\", silent = TRUE)\nt_dist = estimate_parameters(spy$log_return, model = \"t\", silent = TRUE)\nskew_gaussian = estimate_parameters(spy$log_return, model = \"skew_gaussian\", silent = TRUE)\nleverage = estimate_parameters(spy$log_return, model = \"leverage\", silent = TRUE)\n\nsummary(gaussian)\n\ndjb1 <- stochvolTMB::estimate_parameters(data$ir, model = \"gaussian\", silent = TRUE)\nsummary(djb1)\n\ndjb2 <- stochvol::svsample(data$ir, quiet = TRUE)\nsummary(djb2)\n\nslv1 <- stochvol::svsample(sp500b$lr, quiet = TRUE)\nslv1$summary$para\n\nslv2 <- stochvol::svsample(sp500b |> \n                             filter(month <= \"2003-12-01\") |> \n                             pull(lr), quiet = TRUE)\nslv2$summary$para\n\n\n\n\n\n\nsummary(djb1, report = \"transformed\")\ndjb2$summary$para\n\n\nsummary(spy)\nsummary(data)\n\nspy2 <- spy |> \n  mutate(ratio=price / lag(price),\n         lr=log(ratio),\n         lr=ifelse(is.na(lr), log_return, lr))\ngaussian2 = estimate_parameters(spy2$lr, model = \"gaussian\", silent = TRUE)\nsummary(gaussian2)\n\nGSPC |> \n  as_tibble() |> \n  add_column(month = zoo::index(GSPC), .before = 1) |> \n  mutate(price=GSPC.Adjusted,\n         ratio=price / lag(price),\n         lr=log(ratio)) # this is correct\n\ndata\n\n\nstochvol_gauss <- svsample(spy$log_return, quiet = TRUE)\nstochvolTMB_gauss  <- estimate_parameters(spy$log_return, \"gaussian\", silent = TRUE)\n\nstr(stochvol_gauss)\n\nstochvol_gauss$summary$para\nsummary(stochvolTMB_gauss, report = \"transformed\")\n\n\n\n# Model equation for the log returns and volatility\nslv_model <- function(params, returns) {\n  mu = params[1]\n  phi = params[2]\n  sigma_v = params[3]\n  rho = params[4]\n  \n  # Define more of the model here as needed\n  # ...\n  \n  # Placeholder for likelihood calculation\n  # This would need to be adapted to your specific model equations and data\n  likelihood = -sum(dnorm(returns, mean = mu, sd = sigma_v, log = TRUE))\n  \n  return(likelihood)\n}\n\n\n# Initial parameter guesses, lower and upper bounds\ninitial_params <- c(mu = 0.01, phi = 0.5, sigma_v = 0.2, rho = 0)\nlower_bounds <- c(mu = -0.5, phi = 0, sigma_v = 0, rho = -1)\nupper_bounds <- c(mu = 0.5, phi = 1, sigma_v = 1, rho = 1)\n\nslv_model(initial_params, data$ir)\nreturns <- data$ir\n\n# Optimize using constrained MLE\nresult <- mle(minuslogl=slv_model, \n              start = initial_params, \n              method = \"L-BFGS-B\", \n              lower = lower_bounds, \n              upper = upper_bounds, \n              fixed = list(returns = data$ir))\n\n\n\n# new start ----\n\nslv <- function(mu, phi, sigma_v, rho) {\n  nll = -sum(dnorm(returns, mean = mu, sd = sigma_v, log = TRUE))\n  return(nll)\n}\n\nip <- list(mu = 0.01, phi = 0.5, sigma_v = 0.2, rho = 0)\nlb <- list(mu = -0.5, phi = 0, sigma_v = 0, rho = -1)\nub <- list(mu = 0.5, phi = 1, sigma_v = 1, rho = 1)\n\n -sum(dnorm(returns, mean = ip$mu, sd = ip$sigma_v, log = TRUE))\n\n\nslv(ip$mu, ip$phi, ip$sigma_v, ip$rho)\n\n\nresult <- mle(minuslogl=slv, \n              start = ip, \n              method = \"L-BFGS-B\", \n              lower = lb, \n              upper = ub)\n\nsummary(result)\n\n\nresult <- mle(minuslogl=slv_model, \n              start = initial_params, \n              method = \"L-BFGS-B\", \n              lower = lower_bounds, \n              upper = upper_bounds, \n              fixed = list(returns = data$ir))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download monthly adjusted close prices for the S&P 500 index\n# Dec 1955-Dec 2003 was \n\ngetSymbols(\"^GSPC\", from = \"1955-01-01\", to = Sys.Date(), periodicity = \"monthly\", auto.assign = TRUE)\nsummary(GSPC)\n\n# monthly data do not go back far enough so get daily and convert\n# spx <- getSymbols(\"^GSPC\", auto.assign = FALSE, periodicity = \"monthly\", from = \"1950-01-01\")\nspdaily <- getSymbols(\"^GSPC\", auto.assign = FALSE, from = \"1950-01-01\")\n\nspmonth <- spdaily |> \n  as_tibble() |> \n  add_column(date = zoo::index(spdaily), .before = 1) |> \n  mutate(ldom = rollforward(date)) |> \n  filter(date==ldom) |> \n  arrange(date) |> \n  mutate(price=GSPC.Adjusted,\n         lr=log(price / lag(price)))\n\n# log returns: lr=log(price / lag(price)) except that quantmod function fills in 1st observation\n\n# sp500 <- GSPC |> \n#   as_tibble() |> \n#   add_column(date = zoo::index(GSPC), .before = 1) |> \n#   mutate(price=GSPC.Adjusted,\n#          lr=monthlyReturn(GSPC, type = \"log\") |> as.vector())\n# skim(sp500)\n\n# get centered log returns\nperiod <- c(\"1955-12-31\", \"2003-12-31\")\n# period <- c(\"1955-12-31\", \"2023-12-31\")\n# period <- c(\"1950-02-28\", \"2024-03-31\")\n\nlog_returns <- spmonth |> \n  filter(date >= period[1],\n         date <= period[2]) |> \n  mutate(mclr=scale(lr, scale = FALSE) |> c()) |> # mean-centered log return\n  pull(mclr)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}